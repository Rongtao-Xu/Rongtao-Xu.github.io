<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1, minimum-scale=1"
    />
    <title>Rongtao Xu</title>
    <link rel="stylesheet" type="text/css" href="./css/home.css" />
  </head>
  <body>
    <div class="main">
      <div class="main-head">
        <div class="head-pc">
          <div class="name">Rongtao Xu</div>
          <div class="list">
            <div class="list-item"><a href="#news">News</a></div>
            <div class="list-item">
              <a href="#publications">Publications</a>
            </div>
            <div class="list-item"><a href="#awards">Awards</a></div>
            <div class="list-item"><a href="#teaching">Teaching</a></div>
            <div class="list-item">
              <a href="#professional">Professional Service</a>
            </div>
            <div class="list-item">
              <a href="#opportunities">Opportunities</a>
            </div>
          </div>

          <div class="menu">
            <div class="menu-item active">
              <a href="https://hughw19.github.io">Home</a>
            </div>
            <span>/</span>
            <div class="menu-item">
              <a href="https://PKU-EPIC.github.io">Lab</a>
            </div>
          </div>
        </div>
        <div class="head-phone">
          <div class="name">Rongtao Xu</div>
          <svg
            class="menu-phone"
            xmlns="http://www.w3.org/2000/svg"
            width="21"
            height="19"
            viewBox="0 0 21 19"
            fill="none"
          >
            <path
              fill-rule="evenodd"
              clip-rule="evenodd"
              d="M0 1.5C0 0.671573 0.671573 0 1.5 0H19.5C20.3284 0 21 0.671573 21 1.5C21 2.32843 20.3284 3 19.5 3H1.5C0.671573 3 0 2.32843 0 1.5ZM0 9.5C0 8.67157 0.671573 8 1.5 8H19.5C20.3284 8 21 8.67157 21 9.5C21 10.3284 20.3284 11 19.5 11H1.5C0.671573 11 0 10.3284 0 9.5ZM1.5 16C0.671573 16 0 16.6716 0 17.5C0 18.3284 0.671573 19 1.5 19H19.5C20.3284 19 21 18.3284 21 17.5C21 16.6716 20.3284 16 19.5 16H1.5Z"
              fill="#808080"
            />
          </svg>
        </div>
      </div>
      <div class="main-content">
        <div class="main-userinfo">
          <div class="info">
            <div>
              <img class="lazy-load" src="./assets/xurongtao.jpg" alt="" />
              <div class="icon-phone">
                <a
                  href="https://scholar.google.com/citations?user=roCAWkoAAAAJ&hl=en"
                  ><img
                    style="width: 18px; height: 18px"
                    src="./assets/ic_1.svg"
                    alt=""
                /></a>
                <a href="mailto:hewang@pku.edu.cn"
                  ><img
                    style="width: 19px; height: 15px"
                    src="./assets/ic_4.svg"
                    alt=""
                /></a>
                <a href="https://twitter.com/HughWang19"
                  ><img
                    style="width: 19px; height: 16px"
                    src="./assets/ic_3.svg"
                    alt=""
                /></a>
              </div>
            </div>
            <div class="info-right">
              <div class="info-title">Rongtao Xu</div>
              <p>
                Tenure-track Assistant Professor at
                <a href="https://english.pku.edu.cn/">Peking University</a>
              </p>
              <p>
                Director of
                <i>Embodied Perception and InteraCtion (EPIC) Lab</i>
              </p>
              <p>Director of <i> PKU-Galbot Joint Lab of Embodied AI</i></p>
              <div class="icon">
                <a
                  href="https://scholar.google.com/citations?user=roCAWkoAAAAJ&hl=en"
                  ><img
                    style="width: 18px; height: 18px"
                    src="./assets/ic_1.svg"
                    alt=""
                /></a>
                <a href="mailto:hewang@pku.edu.cn"
                  ><img
                    style="width: 19px; height: 15px"
                    src="./assets/ic_4.svg"
                    alt=""
                /></a>
                <a href="https://twitter.com/HughWang19"
                  ><img
                    style="width: 19px; height: 16px"
                    src="./assets/ic_3.svg"
                    alt=""
                /></a>
              </div>
            </div>
          </div>
          <div class="decs">
            <p>
              I am a tenure-track assistant professor in the
              <a href="https://cfcs.pku.edu.cn/english"
                >Center on Frontiers of Computing Studies (CFCS)
              </a>
              at <a href="https://english.pku.edu.cn/">Peking University.</a> I
              founded and lead the
              <i>Embodied Perception and InteraCtion (EPIC) Lab</i> with the
              mission of developing generalizable skills and embodied multimodal
              large model for robots to facilitate embodied AGI.
            </p>
            <p>
              I am also the director of the PKU-Galbot joint lab of Embodied AI
              and the BAAI center of Embodied AI. I have published more than 50
              papers in top conferences and journals of computer vision,
              robotics, and learning, including
              CVPR/ICCV/ECCV/TRO/ICRA/IROS/NeurIPS/ICLR/AAAI. My pioneering work
              on category-level 6D pose estimation, NOCS, received the 2022
              World Artificial Intelligence Conference Youth Outstanding Paper
              (WAICYOP) Award, and my work also received ICCV 2023 best paper
              finalist, ICRA 2023 outstanding manipulation paper award finalist
              and Eurographics 2019 best paper honorable mention.
            </p>
            <p>
              I serve as an associate editor of Image and Vision Computing and
              serve as an area chair in CVPR 2022 and WACV 2022. Prior to
              joining Peking University, I received my Ph.D. degree from
              <a href="https://www.stanford.edu">Stanford University</a> in 2021
              under the advisory of Prof.
              <a href="http://geometry.stanford.edu/member/guibas/index.html"
                >Leonidas J.Guibas</a
              >
              and my Bachelor's degree from
              <a href="http://www.tsinghua.edu.cn/">Tsinghua University</a> in
              2014.
            </p>
          </div>
        </div>
      </div>
      <div class="main-content" style="padding-bottom: 38px">
        <div class="main-swiper" id="videos">
          <div class="swiper-content">
            <div class="slide-source">
              <a href="https://pku-epic.github.io/ASGrasp/">
                <video id="video1" autoplay loop muted>
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech6-1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p1">
                  Material-agnostic generalized grasping technology, capable of
                  handling scenes with completely transparent objects with high
                  success.
                </div>
              </a>
            </div>
            <div class="slide-source">
              <a href="  https://pku-epic.github.io/DexGraspNet/">
                <video id="video2" autoplay loop muted>
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech7-1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p2">
                  《DexGraspNet》ICRA 2023 Best Manipulation Paper
                  Nominee，Million-level dexterous hands Dataset.
                </div>
              </a>
            </div>
            <script>
              var video = document.getElementById('video2');
              var container = document.getElementById('p2');
              container.style.width = video.clientWidth - 56 + 'px';
            </script>
            <div class="slide-source">
              <a href="https://pku-epic.github.io/Open6DOR/">
                <video id="video3" autoplay loop muted>
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech2-1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p3">
                  The world's first open command large model system for object
                  pick-and-place, capable of controlling object orientation:
                  Open6DOR.
                </div>
              </a>
            </div>
            <script>
              var video = document.getElementById('video3');
              var container = document.getElementById('p3');
              container.style.width = video.clientWidth - 56 + 'px';
            </script>
            <div class="slide-source">
              <a href="https://pku-epic.github.io/NaVid/">
                <video autoplay muted loop>
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech3-1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div>
                  The world's first generalized embodied navigation large model:
                  NaVid.
                </div>
              </a>
            </div>
          </div>
          <div class="swiper-content-phone" id="videos1">
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/tech6-1.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  loop
                  muted
                  controlsList="nodownload noPictureInPicture"
                  x5-video-player-type="h5-page"
                  id="video1"
                  poster="https://oss-cn-beijing.galbot.com/online/portal/video/tech6.png"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech6-1.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p1">
                <a href="https://pku-epic.github.io/ASGrasp/"
                  >Material-agnostic generalized grasping technology, capable of
                  handling scenes with completely transparent objects with high
                  success.
                </a>
              </div>
            </div>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/tech7-1.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  id="video2"
                  x5-video-player-type="h5-page"
                  loop
                  muted
                  controlsList="nodownload noPictureInPicture"
                  poster="https://oss-cn-beijing.galbot.com/online/portal/video/tech7.png"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech7-1.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p2">
                <a href="  https://pku-epic.github.io/DexGraspNet/"
                  >《DexGraspNet》ICRA 2023 Best Manipulation Paper
                  Nominee，Million-level dexterous hands Dataset.
                </a>
              </div>
            </div>
            <script>
              var video = document.getElementById('video2');
              var container = document.getElementById('p2');
              container.style.width = video.clientWidth - 56 + 'px';
            </script>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/tech2-1.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />

                <video
                  id="video3"
                  loop
                  controlsList="nodownload noPictureInPicture"
                  muted
                  poster="https://oss-cn-beijing.galbot.com/online/portal/video/tech2.png"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech2-1.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p3">
                <a href="https://pku-epic.github.io/Open6DOR/">
                  The world's first open command large model system for object
                  pick-and-place, capable of controlling object orientation:
                  Open6DOR.
                </a>
              </div>
            </div>
            <script>
              var video = document.getElementById('video3');
              var container = document.getElementById('p3');
              container.style.width = video.clientWidth - 56 + 'px';
            </script>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/tech3-1.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  muted
                  controlsList="nodownload noPictureInPicture"
                  loop
                  poster="https://oss-cn-beijing.galbot.com/online/portal/video/tech3.png"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech3-1.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div>
                <a href="https://pku-epic.github.io/NaVid/">
                  The world's first generalized embodied navigation large model:
                  NaVid.
                </a>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="main-content">
        <div id="news" class="main-news">
          <div class="title">NEWS</div>
          <ul>
            <li>Two papers get accepted to CVPR 2025.</li>
            <li>Two papers get accepted to RA-L.</li>
            <li>Five papers get accepted to ICRA 2025.</li>
            <li>One paper get accepted to TPAMI.</li>
            <li>Four papers get accepted to CoRL 2024.</li>
            <li>
              SAGE,won the <span>Best Paper Award</span> at RSS 2024 SemRob
              Workshop.
            </li>
            <li>Two papers get accepted to IROS 2024.</li>
            <li>Two papers get accepted to RSS 2024.</li>
            <li>
              I accepted an interview from Xinhua News Agency's Economic
              Information Daily titled "<a
                href="https://h.xinhuaxmt.com/vh512/share/12055247?d=134d8e5&channel=weixin"
                >Humanoid Robots Open the Blueprint of Embodied Intelligence</a
              >."
            </li>
            <li>
              I accepted an interview from 36Kr titled "<a
                href="https://mp.weixin.qq.com/s/xE6xmcuDRbhrGtIqMCzGpQ"
                >After the Impact of Large Models on the Humanoid Robot Track, a
                Trillion-Dollar Market's New Narrative</a
              >."
            </li>
            <li>
              I was invited to participate in CCTV-2's "Dialogue" program on the
              theme of "<a
                href="https://tv.cctv.com/2024/05/25/VIDECvVzKGwoW4y2w3bkgTLD240525.shtml?spm=C22284.P87019257382.EMqe9pBD7J5t.63"
                >Humanoid Robots: Coexisting with Humans</a
              >."
            </li>
            <li>Two papers get accepted to CVPR 2024.</li>
            <li>
              Three papers get accepted to ICRA 2024 and one paper gets accepted
              by RAL.
            </li>
            <li>
              Our 3D dexterous grasping policy learning paper, UniDexGrasp++,
              receives<span> ICCV 2023 best paper finalist</span>.
            </li>
            <li>One paper gets accepted to SIGGRAPH Asia 2023.</li>
            <li>
              Three papers get accepted to ICCV 2023 with UniDexGrasp++
              receiving final reviews of <span>all strong accepts</span> (the
              highest ratings).
            </li>
            <li>
              I am invited to be a speaker in
              <a href="https://sites.google.com/view/hands2023/home"
                >HANDS workshop</a
              >
              at ICCV 2023.
            </li>
            <li>
              I am invited to be a speaker in
              <a href="https://generalist-robots.github.io/speakers/"
                >Towards Generalist Robots: Learning Paradigms for Scalable
                Skill Acquisition Workshop </a
              >at CoRL 2023.
            </li>
            <li>
              I am invited to be a speaker in
              <a href="https://sites.google.com/view/rss23-sym"
                >Workshop on Symmetries in Robot Learning</a
              >
              at RSS 2023.
            </li>
            <li>
              Our dexterous grasping synthesis and dataset paper, DexGraspNet,
              is selected as a
              <span
                >finalist of ICRA 2023 outstanding paper in manipulation</span
              >
              (top 1% of submissions).
            </li>
            <li>
              Seven papers get accepted to CVPR 2023 with GAPartNet
              receiving<span> highlight</span> (top 2.5% of submissions) with
              final reviews of <span>all accepts</span> (the highest ratings).
            </li>
            <li>
              I am invited to be an associate editor of
              <a
                href="https://www.sciencedirect.com/journal/image-and-vision-computing"
                >Image and Vision Computing</a
              >, which ranks top 20 in computer vision journals/conferences on
              Google Scholar Metrics.
            </li>
            <li>
              Two papers get accepted to ICLR 2023 with one as<span>
                spotlight</span
              >.
            </li>
            <li>Two papers get accepted to ICRA 2023.</li>
            <li>
              One paper gets accepted to AAAI 2023 as
              <span>oral presentation</span>.
            </li>
            <li>
              My first-author CVPR 2019 oral paper,
              <a
                href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2019_paper.pdf"
                >NOCS</a
              >, received
              <a href="https://www.worldaic.com.cn/activity#q1"
                >2022 World Artificial Intelligence Conference Youth Outstanding
                Paper Award</a
              >. Top 10 from 155 papers published in top-tier AI conferences and
              top journals (Science, Nature Computational Method, etc.) in the
              past 3 years.
            </li>
            <li>One paper gets accepted to T-RO.</li>
            <li>One paper gets accepted to ECCV 2022.</li>
            <li>
              One paper gets accepted to Robotics and Automation Letters (RA-L)
              and IROS 2022.
            </li>
            <li>
              My students and I won the <span>first place </span>in the no
              external annotation track of
              <a href="https://sapien.ucsd.edu/challenges/maniskill2021/"
                >SAPIEN Manipulation Skill Challenge 2021</a
              >
              and will receive $3000 prize and give a winner presentation in
              <a
                href="https://ai-workshops.github.io/generalizable-policy-learning-in-the-physical-world/"
                >ICLR 2022 Generalizable Policy Learning in the Physical World
                Workshop</a
              >.
            </li>
            <li>
              Two papers are selected to <span>oral presentations</span> in CVPR
              2022.
            </li>
            <li>Seven papers get accepted to CVPR 2022.</li>
            <li>...</li>
          </ul>
        </div>
      </div>
      <div class="main-content">
        <div id="publications" class="main-selected">
        <div class="title">SELECTED PUBLICATIONS</div>
        <div class="selected-item">
            <video autoplay muted playsinline loop  preload="auto">
                <source src="./assets/Constraint.mp4" type="video/mp4" />
            </video>
            <div class="item-right">
                <div class="item-title">
                    Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous Environments
                </div>
                <p>
                    Kehan Chen*, Dong An*, Yan Huang, Rongtao Xu, Yifei Su, Yonggen Ling, Ian Reid, 
                    <b>Liang Wang<span>&#8224;</span></b>
                </p>
                <p class="decs">arXiv 2024</p>
                <div class="button">
                    <a href="https://arxiv.org/pdf/2412.10137">Paper</a>
                    <a href="https://chenkehan21.github.io/CA-Nav-project/">Project</a>
                </div>
            </div>
         </div>        
         <div class="selected-item">
            <!-- <img class="lazy-load" src="./assets/xu_3.png" alt="" /> -->
            <!-- <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="./assets/robopanotes.mp4" type="video/mp4">
            </video> -->
            <video id="navid-video" autoplay muted playsinline loop  preload="auto">
              <source src="./assets/Navid.mp4" type="video/mp4" />
            </video>
            <div class="item-right">
                <div class="item-title">
                    NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation
                </div>
                <p>
                    Jiazhao Zhang*, Kunyu Wang*, Rongtao Xu*, Gengze Zhou, Yicong Hong, 
                    Xiaomeng Fang, Qi Wu, Zhizheng Zhang<span>&#8224;</span>, <b>He Wang<span>&#8224;</span></b>
                </p>
                <p class="decs">arXiv 2024</p>
                <div class="button">
                    <a href="https://arxiv.org/pdf/2402.15852">Paper</a>
                    <a href="https://pku-epic.github.io/NaVid/">Project</a>
                </div>
            </div>
          </div>
          <div class="selected-item">
              <img class="lazy-load" src="./assets/xu_4.png" alt="" />
            <div class="item-right">
                <div class="item-title">
                    NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction
                </div>
                <p>
                    Zixuan Gong*, Guangyin Bao*, Qi Zhang†, Zhongwei Wan, Duoqian Miao†, Shoujin Wang, 
                    Lei Zhu, Changwei Wang, Rongtao Xu, Liang Hu, Ke Liu, Yu Zhang
                </p>
                <p class="decs">NeurIPS 2024 <b>(Oral)</b></p>
                <div class="button">
                    <a href="https://arxiv.org/pdf/2410.19452">Paper</a>
                    <a href="https://github.com/gongzix/NeuroClips">Code</a>
                </div>
            </div>
          </div>        
          <div class="selected-item">
            <img class="lazy-load" src="./assets/xu_5.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                DefFusion: Deformable Multimodal Representation Fusion for 3D Semantic Segmentation
              </div>
              <p>
                Rongtao Xu, Changwei Wang, Duzhen Zhang, Man Zhang,
                Shibiao Xu<sup>*</sup>, Weiliang Meng<sup>*</sup>, Xiaopeng Zhang
              </p>
              <p class="decs">ICRA 2024 <b>(Oral)</b></p>
              <div class="button">
                <a href="https://ieeexplore.ieee.org/abstract/document/10610465">Paper</a>
                <!-- <a href="https://zhoues.github.io/Code-as-Monitor/">Project</a> -->
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/xu_6.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Spectral Prompt Tuning: Unveiling Unseen Classes for Zero-Shot Semantic Segmentation
              </div>
              <p>
                Wenhao Xu<sup>*</sup>, Rongtao Xu<sup>*</sup>, Changwei Wang, Shibiao Xu<sup>†</sup>, Li Guo, Man Zhang, Xiaopeng Zhang
              </p>
              <p class="decs">AAAI 2024</p>
              <div class="button">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/download/28456/28888">Paper</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/xu_7.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                RSSFormer: Foreground Saliency Enhancement for Remote Sensing Land-Cover Segmentation
              </div>
              <p>
                Rongtao Xu<sup>*</sup>, Changwei Wang<sup>*</sup>, Jiguang Zhang, Shibiao Xu<sup>†</sup>, Weiliang Meng, Xiaopeng Zhang
              </p>
              <p class="decs">IEEE TIP, 2023 <b>(ESI Highly Cited Paper)</b></p>
              <div class="button">
                <a href="https://ieeexplore.ieee.org/document/10047119">Paper</a>
                <a href="https://github.com/Rongtao-Xu/RepresentationLearning/tree/main/RSSFormer-TIP2023">Code</a>
              </div>
            </div>
          </div>                    
          <div class="selected-item">
            <img class="lazy-load" src="./assets/xu_8.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Attention Weighted Local Descriptors
              </div>
              <p>
                Changwei Wang<sup>*</sup>, Rongtao Xu<sup>*</sup>, Ke Lu, Shibiao Xu<sup>†</sup>, Weiliang Meng, Yuyang Zhang, Bin Fan, Xiaopeng Zhang<sup>†</sup>
              </p>
              <p class="decs">IEEE TPAMI, 2023</p>
              <div class="button">
                <a href="https://ieeexplore.ieee.org/abstract/document/10105519">Paper</a>
                <a href="https://github.com/vignywang/AWDesc">Code</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/xu_9.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Wave-Like Class Activation Map With Representation Fusion for Weakly-Supervised Semantic Segmentation
              </div>
              <p>
                Rongtao Xu<sup>*</sup>, Changwei Wang<sup>*</sup>, Shibiao Xu<sup>†</sup>, Weiliang Meng, Xiaopeng Zhang
              </p>
              <p class="decs">IEEE TMM, 2024 <b>(ESI Highly Cited Paper)</b></p>
              <div class="button">
                <a href="https://ieeexplore.ieee.org/abstract/document/10104136">Paper</a>
                <a href="https://github.com/Rongtao-Xu/RepresentationLearning/tree/main/WaveCAM-TMM2023">Code</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <video autoplay muted playsinline loop  preload="auto">
              <source src="./assets/domainfeat.mp4" type="video/mp4" />
            </video>
            <div class="item-right">
              <div class="item-title">
                DomainFeat: Learning Local Features With Domain Adaptation
              </div>
              <p>
                Rongtao Xu<sup>*</sup>, Changwei Wang<sup>*</sup>, Shibiao Xu<sup>†</sup>, Weiliang Meng, Yuyang Zhang, Bin Fan, Xiaopeng Zhang
              </p>
              <p class="decs">IEEE TCSVT, 2024 <b>(ESI Highly Cited Paper)</b></p>
              <div class="button">
                <a href="https://ieeexplore.ieee.org/abstract/document/10144412">Paper</a>
              </div>
            </div>
          </div>          
          <div class="selected-item">
            <img class="lazy-load" src="./assets/xu_11.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation
              </div>
              <p>
                Rongtao Xu<sup>*</sup>, Changwei Wang<sup>*</sup>, Jiaxi Sun, Shibiao Xu<sup>†</sup>, Weiliang Meng<sup>†</sup>, Xiaopeng Zhang
              </p>
              <p class="decs">AAAI 2023 <b>(Oral)</b></p>
              <div class="button">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/download/25408/25180">Paper</a>
                <a href="https://github.com/Rongtao-Xu/RepresentationLearning/tree/main/SCD-AAAI2023">Code</a>
              </div>
            </div>
          </div>      
          <div class="selected-item">
            <img class="lazy-load" src="./assets/xu_12.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Treating Pseudo-labels Generation as Image Matting for Weakly Supervised Semantic Segmentation
              </div>
              <p>
                Changwei Wang<sup>*</sup>, Rongtao Xu<sup>*</sup>, Shibiao Xu<sup>†</sup>, Weiliang Meng<sup>†</sup>, Xiaopeng Zhang
              </p>
              <p class="decs">ICCV 2023</p>
              <div class="button">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Treating_Pseudo-labels_Generation_as_Image_Matting_for_Weakly_Supervised_Semantic_ICCV_2023_paper.html">Paper</a>
                <a href="https://github.com/Rongtao-Xu/RepresentationLearning/tree/main/Mat-Label-ICCV2023">Code</a>
              </div>
            </div>
          </div>  
          <div class="selected-item">
            <img class="lazy-load" src="./assets/xu_13.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Accurate Lung Nodule Segmentation with Detailed Representation Transfer and Soft Mask Supervision
              </div>
              <p>
                Changwei Wang<sup>*</sup>, Rongtao Xu<sup>*</sup>, Shibiao Xu<sup>†</sup>, Weiliang Meng<sup>†</sup>, Jun Xiao, Xiaopeng Zhang
              </p>
              <p class="decs">IEEE TNNLS, 2023</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2007.14556">Paper</a>
                <a href="https://github.com/Rongtao-Xu/RepresentationLearning/tree/main/DSNet-TMI2024">Code</a>
              </div>
            </div>
          </div>                      
        </div>
      </div>
      <div class="main-content">
        <div id="awards" class="main-news">
          <div class="title">AWARDS</div>
          <ul>
            <li>ICCV 2023 Best Paper Finalist</li>
            <li>ICRA 2023 Outstanding Manipulation Paper Finalist</li>
            <li>
              2022 World Artificial Intelligence Conference Youth Outstanding
              Paper Award
            </li>
            <li>Eurographics 2019 Best Paper Honorable Mention</li>
            <li>
              1st prize winner of
              <a href="https://sapien.ucsd.edu/challenges/maniskill2021/"
                >SAPIEN ManiSkill Challenge 2021</a
              >
              (no external annotation track)
            </li>
            <li></li>
          </ul>
        </div>
      </div>
      <div class="main-content">
        <div id="teaching" class="main-news">
          <div class="title">TEACHING</div>
          <ul>
            <li>
              Undergraduate course:
              <a href="https://pku-epic.github.io/Intro2CV_2024/"
                >Introduction to Computer Vision</a
              >, Spring 2024
            </li>
            <li>
              Graduate course:
              <a href="https://hughw19.github.io/RVAL/"
                >Robot Vision and Learning - From a Perspective of Embodied
                AI</a
              >, Fall 2023
            </li>
            <li></li>
          </ul>
        </div>
      </div>
      <div class="main-content">
        <div id="professional" class="main-news">
          <div class="title">PROFESSIONAL SERVICE</div>
          <ul>
            <li>
              Associate Editor:
              <a
                href="https://www.sciencedirect.com/journal/image-and-vision-computing"
                >Image and Vision Computing</a
              >
            </li>
            <li>
              Area chair (AC):
              <br />Conferences: CVPR 2022, WACV 2022 <br />Seminars:
              <a href="http://valser.org">VALSE</a>
            </li>
            <li>
              Program committee/reviewer:
              <br />Conferences: CVPR, ICCV, ECCV, NeurIPS, ICLR, AAAI,
              SIGGRAPH, RSS, IROS, ICRA <br />Journals: IEEE TPAMI, IEEE RAL
            </li>
            <li></li>
          </ul>
        </div>
      </div>
      <div class="main-content border-none">
        <div id="opportunities" class="main-news">
          <div class="title">OPPORTUNITIES</div>
          <p>
            We are actively looking for interns,full-time employees, master/PhD
            students, and postdocs. Feel free to contact me if you are
            interested in my research or potential collaborations.
          </p>
          <ul>
            <li>
              For graduate school applicants, we have two openings for PhD
              students (in addition to master/PhD openings for foreigners) each
              year, and please contact me at least one year prior to the
              application deadline.
            </li>
            <li>
              For visiting students or research interns, we welcome undergradute
              and graduate students from top universities all world wide to
              apply for >6 months research internship. Our interns have
              published many top-tier conference/journal papers and have been
              admitted to PhD/MS programs in Stanford, CMU, UCLA, UCSD, etc.
            </li>
            <li>
              In Embodied AI center of
              <a href="https://www.baai.ac.cn/">BAAI</a> and our Embodied AI
              startup, <a href="https://www.galbot.com">Galbot</a>, we also
              actively hire full-time research scientists, engineers and
              interns. Email me if you are interested.
            </li>
            <li></li>
          </ul>
        </div>
      </div>
      <div class="modal">
        <div class="modal-close">
          <img src="./assets/close.svg" alt="" class="close" />
        </div>
        <div class="modal-content">
          <div class="modal-content-item" id="link1">
            <a href="#news">News</a>
          </div>
          <div class="modal-content-item" id="link2">
            <a href="#publications">Publications</a>
          </div>
          <div class="modal-content-item" id="link3">
            <a href="#awards">Awards</a>
          </div>
          <div class="modal-content-item" id="link4">
            <a href="#teaching">Teaching</a>
          </div>
          <div class="modal-content-item" id="link5">
            <a href="#professional">Professional Service</a>
          </div>
          <div class="modal-content-item" id="link6">
            <a href="#opportunities">Opportunities</a>
          </div>
        </div>
        <div class="modal-footer">
          <div class="modal-footer-item footer-item-active">
            <a href="https://hughw19.github.io">Home</a>
          </div>
          <span>/</span>
          <div class="modal-footer-item">
            <a href="https://PKU-EPIC.github.io">Lab</a>
          </div>
        </div>
      </div>
    </div>
    <script
      src="https://code.jquery.com/jquery-3.7.1.js"
      integrity="sha256-eKhayi8LEQwp4NKxN+CfCh+3qOVUtJn3QNZ0TciWLP4="
      crossorigin="anonymous"
    ></script>
    <script>
      document
        .getElementById('link1')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('news');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link2')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('publications');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link3')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('awards');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link4')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('teaching');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link5')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('professional');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link6')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('opportunities');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
    </script>
    <script type="text/javascript">
      function onresizeFun() {
        if (window.innerWidth <= 768) {
          let width = document.documentElement.clientWidth;
          // 假设设计稿宽度为750px
          // 假设已知根元素我们设置为100px（这里设置100方便后续我们好计算）
          // 动态设置根元素html的fontSize
          document.documentElement.style.fontSize = 100 * (width / 430) + 'px';
          $('.head-pc').css('display', 'none');
          $('.icon').css('display', 'none');
          $('.swiper-content').css('display', 'none');
          $('.head-phone').css('display', 'flex');
          $('.icon-phone').css('display', 'flex');
          $('.swiper-content-phone').css('display', 'block');
        } else {
          $('.head-pc').css('display', 'flex');
          $('.icon').css('display', 'flex');
          $('.swiper-content').css('display', 'block');
          $('.swiper-content-phone').css('display', 'none');
          $('.head-phone').css('display', 'none');
          $('.icon-phone').css('display', 'none');
        }
        $('.main').css('display', 'block');
      }

      // pc 视频自动播放
      // function autoPlayVideo() {
      //   const videosContainer = document.getElementById('videos');
      //   const videos = document.querySelectorAll('video');

      //   // 检查视频是否在可见范围内
      //   function checkVisibility(video) {
      //     const rect = video.getBoundingClientRect();
      //     const containerRect = videosContainer.getBoundingClientRect();
      //     return (
      //       rect.top >= containerRect.top &&
      //       rect.left >= containerRect.left &&
      //       rect.bottom <= containerRect.bottom &&
      //       rect.right <= containerRect.right
      //     );
      //   }

      //   // 控制视频的播放和暂停
      //   function controlVideoPlayback() {
      //     videos.forEach((video) => {
      //       if (checkVisibility(video)) {
      //         video.play();
      //       } else {
      //         video.pause();
      //       }
      //     });
      //   }

      //   // 初始检查
      //   controlVideoPlayback();

      //   // 滚动事件监听
      //   videosContainer.addEventListener('scroll', controlVideoPlayback);
      //   window.addEventListener('resize', controlVideoPlayback);
      // }
      function autoPlayVideo() {
        const videosContainer = document.getElementById('videos');
        const videos = videosContainer.querySelectorAll('video'); // 修复点

        function checkVisibility(video) {
          const rect = video.getBoundingClientRect();
          const containerRect = videosContainer.getBoundingClientRect();
          return (
            rect.top >= containerRect.top &&
            rect.left >= containerRect.left &&
            rect.bottom <= containerRect.bottom &&
            rect.right <= containerRect.right
          );
        }

        function controlVideoPlayback() {
          videos.forEach((video) => {
            if (checkVisibility(video)) {
              video.play();
            } else {
              video.pause();
            }
          });
        }

        controlVideoPlayback();

        videosContainer.addEventListener('scroll', controlVideoPlayback);
        window.addEventListener('resize', controlVideoPlayback);
      }

      onresizeFun();
      window.addEventListener('resize', onresizeFun);

      document.addEventListener('DOMContentLoaded', function () {
        // 图片懒加载
        // 图片懒加载
        //const observer = new IntersectionObserver((entries) => {
        //  entries.forEach((entry) => {
        //    if (entry.isIntersecting) {
        //      const img = entry.target;
        //      img.src = img.getAttribute('data-src');
        //      observer.unobserve(img);
        //     }
        //   });
        //  });

        //document.querySelectorAll('img.lazy-load').forEach((img) => {
        // observer.observe(img);
        // });

        // 菜单点击事件
        $('.menu-phone').click(() => {
          $('.modal').addClass('modal-active');
          $('body').addClass('noscroll');
        });
        $('.modal-close').click(() => {
          $('.modal').removeClass('modal-active');
          $('body').removeClass('noscroll');
        });
        $('.modal-content-item').click(function () {
          // Remove the 'item-active' class from all siblings
          $(this).siblings('.modal-content-item').removeClass('item-active');
          // Add the 'item-active' class to the clicked element
          $(this).addClass('item-active');
          $('.modal').removeClass('modal-active');
          $('body').removeClass('noscroll');
        });

        if (window.innerWidth > 768) {
          autoPlayVideo();
        }
      });

      function hideshow(a, which) {
        console.log(which);
        if (!document.getElementById) return;
        if (which.style.display == 'block') {
          which.style.display = 'none';
          a.style.background = 'none';
        } else {
          which.style.display = 'block';
          a.style.background = 'rgba(189,220,255,0.30)';
        }
      }
      document.addEventListener("DOMContentLoaded", function () {
        const navidvideo = document.getElementById("navid-video");
        if (navidvideo) {
          navidvideo.playbackRate = 4.5; // 设置为 4.5 倍速播放
        }
      });
      // document.addEventListener('DOMContentLoaded', function () {
      //   const myvideo = document.querySelector('video[src="./assets/robopanotes.mp4"]');
      //   if (myvideo) {
      //     myvideo.play().catch(err => {
      //       console.log('Autoplay failed:', err);
      //     });
      //   }
      // });

    </script>
  </body>
</html>
